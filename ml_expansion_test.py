#!/usr/bin/env python3
"""
PROMISE AI - ML Expansion & Azure OpenAI Integration Testing
Tests 35+ ML models, Azure OpenAI integration, and Oracle compatibility
"""

import requests
import json
import sys
import time
from datetime import datetime

# Backend URL from environment
BACKEND_URL = "https://model-wizard-2.preview.emergentagent.com/api"

def print_section(title):
    """Print formatted section header"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80)

def print_test(test_name):
    """Print test name"""
    print(f"\n--- {test_name} ---")

def test_api_health():
    """Test 0: API Health Check"""
    print_test("Test 0: API Health Check")
    try:
        response = requests.get(f"{BACKEND_URL}/", timeout=10)
        print(f"Status Code: {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ API is healthy")
            print(f"   Version: {data.get('version', 'Unknown')}")
            return True
        else:
            print(f"‚ùå API health check failed")
            return False
    except Exception as e:
        print(f"‚ùå API health check exception: {str(e)}")
        return False

def test_model_catalog():
    """Test 1: Model Catalog - Verify 35+ models"""
    print_test("Test 1: Model Catalog (35+ Models)")
    try:
        response = requests.get(f"{BACKEND_URL}/models/catalog", timeout=10)
        print(f"Status Code: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            catalog = data.get('catalog', {})
            total_models = data.get('total_models', 0)
            categories = data.get('categories', [])
            
            print(f"‚úÖ Model catalog retrieved")
            print(f"   Total Models: {total_models}")
            print(f"   Categories: {', '.join(categories)}")
            
            # Verify each category
            for category, models in catalog.items():
                print(f"   {category.capitalize()}: {len(models)} models")
                if models:
                    sample = models[0]
                    print(f"      Sample: {sample.get('name')} - {sample.get('description')[:50]}...")
            
            # Verify we have 35+ models
            if total_models >= 35:
                print(f"‚úÖ PASSED: {total_models} models available (target: 35+)")
                return True, total_models
            else:
                print(f"‚ùå FAILED: Only {total_models} models (expected 35+)")
                return False, total_models
        else:
            print(f"‚ùå Model catalog failed: {response.status_code}")
            return False, 0
            
    except Exception as e:
        print(f"‚ùå Model catalog exception: {str(e)}")
        return False, 0

def test_available_models_by_type():
    """Test 2: Available Models by Problem Type"""
    print_test("Test 2: Available Models by Problem Type")
    
    problem_types = ['classification', 'regression', 'clustering', 'dimensionality', 'anomaly']
    results = {}
    all_passed = True
    
    for problem_type in problem_types:
        try:
            response = requests.get(
                f"{BACKEND_URL}/models/available",
                params={'problem_type': problem_type},
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                count = data.get('count', 0)
                models = data.get('models', [])
                
                print(f"   {problem_type.capitalize()}: {count} models")
                if models:
                    print(f"      Examples: {', '.join([m.get('name', 'Unknown') for m in models[:3]])}")
                
                results[problem_type] = count
                
                # Verify minimum counts
                min_expected = {
                    'classification': 10,
                    'regression': 10,
                    'clustering': 3,
                    'dimensionality': 2,
                    'anomaly': 3
                }
                
                if count >= min_expected.get(problem_type, 1):
                    print(f"      ‚úÖ Sufficient models for {problem_type}")
                else:
                    print(f"      ‚ùå Insufficient models for {problem_type} (expected {min_expected[problem_type]}+)")
                    all_passed = False
            else:
                print(f"   ‚ùå Failed to get {problem_type} models: {response.status_code}")
                all_passed = False
                
        except Exception as e:
            print(f"   ‚ùå Exception for {problem_type}: {str(e)}")
            all_passed = False
    
    if all_passed:
        print(f"\n‚úÖ PASSED: All problem types have sufficient models")
        print(f"   Summary: {results}")
        return True, results
    else:
        print(f"\n‚ùå FAILED: Some problem types have insufficient models")
        return False, results

def test_model_recommendations():
    """Test 3: AI Model Recommendations"""
    print_test("Test 3: AI Model Recommendations")
    
    test_cases = [
        {
            'name': 'Classification - Small Dataset',
            'problem_type': 'classification',
            'data_summary': {
                'row_count': 500,
                'feature_count': 10,
                'missing_percentage': 2
            }
        },
        {
            'name': 'Regression - Large Dataset',
            'problem_type': 'regression',
            'data_summary': {
                'row_count': 50000,
                'feature_count': 25,
                'missing_percentage': 5
            }
        }
    ]
    
    all_passed = True
    
    for test_case in test_cases:
        print(f"\n   Testing: {test_case['name']}")
        try:
            response = requests.post(
                f"{BACKEND_URL}/models/recommend",
                json=test_case,
                timeout=15
            )
            
            print(f"   Status Code: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                recommendations = data.get('recommendations', [])
                reasoning = data.get('reasoning', '')
                
                print(f"   ‚úÖ Recommendations received")
                if isinstance(recommendations, list):
                    print(f"      Recommended models: {', '.join(recommendations[:5])}")
                else:
                    print(f"      Recommendations: {recommendations}")
                
                if reasoning:
                    print(f"      Reasoning: {reasoning[:100]}...")
            else:
                print(f"   ‚ùå Recommendation failed: {response.status_code}")
                all_passed = False
                
        except Exception as e:
            print(f"   ‚ùå Exception: {str(e)}")
            all_passed = False
    
    if all_passed:
        print(f"\n‚úÖ PASSED: Model recommendations working")
        return True
    else:
        print(f"\n‚ùå FAILED: Model recommendations have issues")
        return False

def test_enhanced_analysis_with_model_selection():
    """Test 4: Enhanced Analysis with Model Selection"""
    print_test("Test 4: Enhanced Analysis with Model Selection")
    
    # First, get a dataset
    try:
        datasets_response = requests.get(f"{BACKEND_URL}/datasets", timeout=10)
        if datasets_response.status_code != 200:
            print("‚ùå Cannot retrieve datasets")
            return False
        
        datasets = datasets_response.json().get('datasets', [])
        if not datasets:
            print("‚ùå No datasets available for testing")
            return False
        
        # Use first dataset
        dataset_id = datasets[0].get('id')
        dataset_name = datasets[0].get('name')
        print(f"   Using dataset: {dataset_name} (ID: {dataset_id})")
        
        # Test with classification models
        print(f"\n   Testing Classification with Model Selection...")
        payload = {
            'dataset_id': dataset_id,
            'problem_type': 'classification',
            'selected_models': ['logistic_regression', 'random_forest', 'svm']
        }
        
        response = requests.post(
            f"{BACKEND_URL}/analysis/holistic",
            json=payload,
            timeout=90
        )
        
        print(f"   Status Code: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            ml_models = data.get('ml_models', [])
            problem_type = data.get('problem_type', 'unknown')
            
            print(f"   ‚úÖ Analysis completed")
            print(f"      Problem Type: {problem_type}")
            print(f"      Models Trained: {len(ml_models)}")
            
            # Verify selected models were used
            if ml_models:
                model_names = [m.get('model_name', '') for m in ml_models]
                print(f"      Model Names: {', '.join(model_names)}")
                
                # Check if classification metrics are present
                sample_model = ml_models[0]
                has_classification_metrics = any(
                    key in sample_model for key in ['accuracy', 'precision', 'recall', 'f1_score']
                )
                
                if has_classification_metrics:
                    print(f"      ‚úÖ Classification metrics present")
                    print(f"         Sample: {sample_model.get('model_name')} - Accuracy: {sample_model.get('accuracy', 'N/A')}")
                else:
                    print(f"      ‚ö†Ô∏è Classification metrics missing")
            
            # Test with regression models
            print(f"\n   Testing Regression with Model Selection...")
            payload_reg = {
                'dataset_id': dataset_id,
                'problem_type': 'regression',
                'selected_models': ['linear_regression', 'ridge', 'random_forest_reg']
            }
            
            response_reg = requests.post(
                f"{BACKEND_URL}/analysis/holistic",
                json=payload_reg,
                timeout=90
            )
            
            if response_reg.status_code == 200:
                data_reg = response_reg.json()
                ml_models_reg = data_reg.get('ml_models', [])
                
                print(f"   ‚úÖ Regression analysis completed")
                print(f"      Models Trained: {len(ml_models_reg)}")
                
                if ml_models_reg:
                    sample_reg = ml_models_reg[0]
                    has_regression_metrics = any(
                        key in sample_reg for key in ['r2_score', 'rmse', 'mae']
                    )
                    
                    if has_regression_metrics:
                        print(f"      ‚úÖ Regression metrics present")
                        print(f"         Sample: {sample_reg.get('model_name')} - R¬≤: {sample_reg.get('r2_score', 'N/A')}")
                    else:
                        print(f"      ‚ö†Ô∏è Regression metrics missing")
                
                print(f"\n‚úÖ PASSED: Enhanced analysis with model selection working")
                return True
            else:
                print(f"   ‚ùå Regression analysis failed: {response_reg.status_code}")
                return False
        else:
            print(f"   ‚ùå Classification analysis failed: {response.status_code}")
            try:
                error = response.json()
                print(f"      Error: {error}")
            except:
                print(f"      Error: {response.text[:200]}")
            return False
            
    except Exception as e:
        print(f"‚ùå Enhanced analysis exception: {str(e)}")
        return False

def test_azure_openai_chat():
    """Test 5: Azure OpenAI Chat Integration"""
    print_test("Test 5: Azure OpenAI Chat Integration")
    
    try:
        # Get a dataset
        datasets_response = requests.get(f"{BACKEND_URL}/datasets", timeout=10)
        if datasets_response.status_code != 200:
            print("‚ùå Cannot retrieve datasets")
            return False
        
        datasets = datasets_response.json().get('datasets', [])
        if not datasets:
            print("‚ùå No datasets available")
            return False
        
        dataset_id = datasets[0].get('id')
        dataset_name = datasets[0].get('name')
        print(f"   Using dataset: {dataset_name}")
        
        # Test chat with data
        test_messages = [
            "What are the key insights from this data?",
            "Show me a scatter plot",
            "What patterns do you see?"
        ]
        
        all_passed = True
        
        for message in test_messages:
            print(f"\n   Testing message: '{message}'")
            
            payload = {
                'dataset_id': dataset_id,
                'message': message,
                'conversation_history': []
            }
            
            response = requests.post(
                f"{BACKEND_URL}/analysis/chat-action",
                json=payload,
                timeout=30
            )
            
            print(f"   Status Code: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                response_type = data.get('type', 'unknown')
                
                print(f"   ‚úÖ Chat response received")
                print(f"      Response Type: {response_type}")
                
                if response_type == 'text':
                    text = data.get('message', '')
                    print(f"      Message: {text[:100]}...")
                elif response_type == 'chart':
                    print(f"      Chart data present: {bool(data.get('data'))}")
                elif response_type == 'error':
                    print(f"      Error: {data.get('message', 'Unknown error')}")
            else:
                print(f"   ‚ö†Ô∏è Chat request failed: {response.status_code}")
                # Don't fail the test if Azure OpenAI is not configured
                if response.status_code == 500:
                    print(f"      Note: This may be expected if Azure OpenAI is not fully configured")
        
        print(f"\n‚úÖ PASSED: Azure OpenAI chat integration tested (may have graceful fallbacks)")
        return True
        
    except Exception as e:
        print(f"‚ùå Azure OpenAI chat exception: {str(e)}")
        return False

def test_oracle_database_operations():
    """Test 6: Oracle Database Compatibility"""
    print_test("Test 6: Oracle Database Compatibility")
    
    try:
        # Check current database
        config_response = requests.get(f"{BACKEND_URL}/config/current-database", timeout=10)
        
        if config_response.status_code == 200:
            current_db = config_response.json().get('current_database')
            print(f"   Current Database: {current_db}")
            
            # Test dataset operations
            datasets_response = requests.get(f"{BACKEND_URL}/datasets", timeout=10)
            
            if datasets_response.status_code == 200:
                datasets = datasets_response.json().get('datasets', [])
                print(f"   ‚úÖ Dataset retrieval working")
                print(f"      Datasets found: {len(datasets)}")
                
                if datasets:
                    sample = datasets[0]
                    print(f"      Sample dataset: {sample.get('name')}")
                    print(f"         Rows: {sample.get('row_count', 'N/A')}")
                    print(f"         Columns: {sample.get('column_count', 'N/A')}")
                
                print(f"\n‚úÖ PASSED: Oracle database operations working")
                return True
            else:
                print(f"   ‚ùå Dataset retrieval failed: {datasets_response.status_code}")
                return False
        else:
            print(f"   ‚ùå Database config failed: {config_response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Oracle database test exception: {str(e)}")
        return False

def test_existing_features_regression():
    """Test 7: Existing Features Regression Test"""
    print_test("Test 7: Existing Features Regression Test")
    
    tests = {
        'datasets_endpoint': f"{BACKEND_URL}/datasets",
        'config_endpoint': f"{BACKEND_URL}/config/current-database",
    }
    
    all_passed = True
    
    for test_name, endpoint in tests.items():
        try:
            response = requests.get(endpoint, timeout=10)
            
            if response.status_code == 200:
                print(f"   ‚úÖ {test_name}: Working")
            else:
                print(f"   ‚ùå {test_name}: Failed ({response.status_code})")
                all_passed = False
                
        except Exception as e:
            print(f"   ‚ùå {test_name}: Exception - {str(e)}")
            all_passed = False
    
    if all_passed:
        print(f"\n‚úÖ PASSED: Existing features working correctly")
        return True
    else:
        print(f"\n‚ùå FAILED: Some existing features have issues")
        return False

def main():
    """Run all ML Expansion & Azure OpenAI tests"""
    print("\n" + "="*80)
    print("  üöÄ PROMISE AI - ML EXPANSION & AZURE OPENAI INTEGRATION TESTING")
    print("="*80)
    print(f"Backend URL: {BACKEND_URL}")
    print(f"Test Time: {datetime.now().isoformat()}")
    print("="*80)
    
    results = {}
    
    # Test 0: API Health
    print_section("Test 0: API Health Check")
    results['api_health'] = test_api_health()
    
    if not results['api_health']:
        print("\n‚ùå API is not accessible. Stopping tests.")
        return False
    
    # Test 1: Model Catalog
    print_section("Test 1: Model Catalog (35+ Models)")
    passed, total_models = test_model_catalog()
    results['model_catalog'] = passed
    
    # Test 2: Available Models by Type
    print_section("Test 2: Available Models by Problem Type")
    passed, model_counts = test_available_models_by_type()
    results['available_models'] = passed
    
    # Test 3: Model Recommendations
    print_section("Test 3: AI Model Recommendations")
    results['model_recommendations'] = test_model_recommendations()
    
    # Test 4: Enhanced Analysis with Model Selection
    print_section("Test 4: Enhanced Analysis with Model Selection")
    results['enhanced_analysis'] = test_enhanced_analysis_with_model_selection()
    
    # Test 5: Azure OpenAI Chat
    print_section("Test 5: Azure OpenAI Chat Integration")
    results['azure_openai_chat'] = test_azure_openai_chat()
    
    # Test 6: Oracle Database
    print_section("Test 6: Oracle Database Compatibility")
    results['oracle_database'] = test_oracle_database_operations()
    
    # Test 7: Existing Features Regression
    print_section("Test 7: Existing Features Regression Test")
    results['existing_features'] = test_existing_features_regression()
    
    # Summary
    print("\n" + "="*80)
    print("  üìä TEST SUMMARY")
    print("="*80)
    
    passed_tests = sum(1 for result in results.values() if result)
    total_tests = len(results)
    
    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{test_name.replace('_', ' ').title()}: {status}")
    
    print(f"\nOverall: {passed_tests}/{total_tests} tests passed")
    print(f"Success Rate: {(passed_tests/total_tests)*100:.1f}%")
    
    # Detailed Findings
    print("\n" + "="*80)
    print("  üîç KEY FINDINGS")
    print("="*80)
    
    if results.get('model_catalog'):
        print(f"‚úÖ Model catalog with 35+ models available")
    else:
        print(f"‚ùå Model catalog has issues")
    
    if results.get('available_models'):
        print(f"‚úÖ All problem types have sufficient models")
    else:
        print(f"‚ùå Some problem types lack sufficient models")
    
    if results.get('model_recommendations'):
        print(f"‚úÖ AI model recommendations working")
    else:
        print(f"‚ùå Model recommendations need attention")
    
    if results.get('enhanced_analysis'):
        print(f"‚úÖ Enhanced analysis with model selection working")
    else:
        print(f"‚ùå Enhanced analysis has issues")
    
    if results.get('azure_openai_chat'):
        print(f"‚úÖ Azure OpenAI chat integration tested")
    else:
        print(f"‚ö†Ô∏è Azure OpenAI chat may need configuration")
    
    if results.get('oracle_database'):
        print(f"‚úÖ Oracle database operations working")
    else:
        print(f"‚ùå Oracle database has issues")
    
    if results.get('existing_features'):
        print(f"‚úÖ No regression in existing features")
    else:
        print(f"‚ùå Some existing features broken")
    
    # Final Status
    print("\n" + "="*80)
    if passed_tests == total_tests:
        print("  üéâ ALL TESTS PASSED - ML EXPANSION READY FOR PRODUCTION")
    elif passed_tests >= total_tests * 0.8:
        print("  ‚úÖ MOST TESTS PASSED - MINOR ISSUES TO ADDRESS")
    else:
        print("  ‚ö†Ô∏è SIGNIFICANT ISSUES DETECTED - REQUIRES ATTENTION")
    print("="*80)
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
