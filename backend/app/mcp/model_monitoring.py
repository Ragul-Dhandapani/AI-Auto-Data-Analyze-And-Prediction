"""
Real-Time Model Monitoring & Drift Detection

Monitors:
1. Data drift - Input distribution changes
2. Concept drift - Relationship between features and target changes
3. Performance degradation - Model accuracy drops
4. Prediction distribution - Output changes

This is PRODUCTION-GRADE - Copilot doesn't give you this!
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any
import json
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class ModelMonitor:
    """
    Monitor model performance and detect drift in production
    """
    
    def __init__(self, model_id: str, monitoring_dir: str = "/app/backend/monitoring"):
        self.model_id = model_id
        self.monitoring_dir = Path(monitoring_dir) / model_id
        self.monitoring_dir.mkdir(parents=True, exist_ok=True)
        
        # Load or initialize monitoring state
        self.state_file = self.monitoring_dir / "monitoring_state.json"
        self.state = self._load_state()
    
    def _load_state(self):
        """Load monitoring state"""
        if self.state_file.exists():
            with open(self.state_file, 'r') as f:
                return json.load(f)
        return {
            'baseline_stats': None,
            'predictions_log': [],
            'performance_log': [],
            'drift_alerts': [],
            'created_at': datetime.now().isoformat()
        }
    
    def _save_state(self):
        """Save monitoring state"""
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
    
    def set_baseline(self, X_train: np.ndarray, y_train: np.ndarray, feature_names: List[str]):
        """
        Set baseline statistics from training data
        
        Used to detect drift later
        """
        self.state['baseline_stats'] = {
            'feature_means': X_train.mean(axis=0).tolist(),
            'feature_stds': X_train.std(axis=0).tolist(),
            'feature_mins': X_train.min(axis=0).tolist(),
            'feature_maxs': X_train.max(axis=0).tolist(),
            'target_mean': float(np.mean(y_train)),
            'target_std': float(np.std(y_train)),
            'target_min': float(np.min(y_train)),
            'target_max': float(np.max(y_train)),
            'feature_names': feature_names,
            'n_samples': len(X_train),
            'set_at': datetime.now().isoformat()
        }
        self._save_state()
        logger.info(f"âœ… Baseline set with {len(X_train)} samples")
    
    def log_predictions(
        self, 
        X: np.ndarray, 
        predictions: np.ndarray, 
        actuals: np.ndarray = None,
        metadata: Dict = None
    ):
        """
        Log predictions for monitoring
        
        Args:
            X: Input features
            predictions: Model predictions
            actuals: True values (if available)
            metadata: Additional info (timestamp, user_id, etc.)
        """
        
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'n_predictions': len(predictions),
            'prediction_mean': float(np.mean(predictions)),
            'prediction_std': float(np.std(predictions)),
            'prediction_min': float(np.min(predictions)),
            'prediction_max': float(np.max(predictions)),
            'feature_means': X.mean(axis=0).tolist(),
            'metadata': metadata or {}
        }
        
        # If actuals available, calculate performance
        if actuals is not None:
            log_entry['actuals_available'] = True
            log_entry['mae'] = float(np.mean(np.abs(predictions - actuals)))
            log_entry['mse'] = float(np.mean((predictions - actuals) ** 2))
        
        self.state['predictions_log'].append(log_entry)
        
        # Keep only last 1000 entries
        if len(self.state['predictions_log']) > 1000:
            self.state['predictions_log'] = self.state['predictions_log'][-1000:]
        
        self._save_state()
    
    def detect_data_drift(self, X_new: np.ndarray, threshold: float = 0.1) -> Dict:
        """
        Detect data drift using distribution comparison
        
        Method: Compare feature distributions with baseline
        """
        if not self.state['baseline_stats']:
            return {'error': 'No baseline set'}
        
        baseline = self.state['baseline_stats']
        drift_detected = []
        
        for i, feature_name in enumerate(baseline['feature_names']):
            # Calculate distribution shift
            baseline_mean = baseline['feature_means'][i]
            baseline_std = baseline['feature_stds'][i]
            
            current_mean = X_new[:, i].mean()
            current_std = X_new[:, i].std()
            
            # Normalized difference
            mean_shift = abs(current_mean - baseline_mean) / (baseline_std + 1e-10)
            std_shift = abs(current_std - baseline_std) / (baseline_std + 1e-10)
            
            if mean_shift > threshold or std_shift > threshold:
                drift_detected.append({
                    'feature': feature_name,
                    'baseline_mean': baseline_mean,
                    'current_mean': float(current_mean),
                    'mean_shift': float(mean_shift),
                    'baseline_std': baseline_std,
                    'current_std': float(current_std),
                    'std_shift': float(std_shift),
                    'severity': 'high' if mean_shift > threshold * 2 else 'medium'
                })
        
        result = {
            'drift_detected': len(drift_detected) > 0,
            'n_features_drifted': len(drift_detected),
            'total_features': len(baseline['feature_names']),
            'drift_percentage': len(drift_detected) / len(baseline['feature_names']) * 100,
            'drifted_features': drift_detected,
            'timestamp': datetime.now().isoformat()
        }
        
        # Log alert if significant drift
        if result['drift_percentage'] > 20:
            self.state['drift_alerts'].append({
                'type': 'data_drift',
                'severity': 'high',
                'message': f"{result['drift_percentage']:.1f}% of features have drifted",
                'timestamp': datetime.now().isoformat(),
                'details': result
            })
            self._save_state()
        
        return result
    
    def detect_performance_degradation(self, window_size: int = 100) -> Dict:
        """
        Detect if model performance is degrading over time
        
        Requires predictions with actuals
        """
        logs = self.state['predictions_log']
        
        # Filter logs with actuals
        logs_with_actuals = [log for log in logs if log.get('actuals_available')]
        
        if len(logs_with_actuals) < window_size:
            return {'error': 'Not enough data with actuals'}
        
        recent_logs = logs_with_actuals[-window_size:]
        older_logs = logs_with_actuals[-2*window_size:-window_size] if len(logs_with_actuals) >= 2*window_size else logs_with_actuals[:window_size]
        
        recent_mae = np.mean([log['mae'] for log in recent_logs])
        older_mae = np.mean([log['mae'] for log in older_logs])
        
        performance_change = ((recent_mae - older_mae) / older_mae) * 100
        
        result = {
            'performance_degrading': performance_change > 10,  # >10% increase in error
            'recent_mae': float(recent_mae),
            'baseline_mae': float(older_mae),
            'change_percent': float(performance_change),
            'severity': 'high' if performance_change > 25 else 'medium' if performance_change > 10 else 'low',
            'timestamp': datetime.now().isoformat()
        }
        
        # Log alert if degrading
        if result['performance_degrading']:
            self.state['drift_alerts'].append({
                'type': 'performance_degradation',
                'severity': result['severity'],
                'message': f"Model performance degraded by {performance_change:.1f}%",
                'timestamp': datetime.now().isoformat(),
                'details': result
            })
            self._save_state()
        
        return result
    
    def get_monitoring_dashboard(self) -> Dict:
        """
        Get comprehensive monitoring dashboard data
        """
        logs = self.state['predictions_log']
        
        if not logs:
            return {'error': 'No predictions logged yet'}
        
        # Recent statistics
        recent_logs = logs[-100:]
        
        return {
            'model_id': self.model_id,
            'monitoring_since': self.state.get('created_at'),
            'total_predictions': sum(log['n_predictions'] for log in logs),
            'baseline_set': self.state['baseline_stats'] is not None,
            'recent_stats': {
                'last_100_predictions': {
                    'mean': float(np.mean([log['prediction_mean'] for log in recent_logs])),
                    'std': float(np.mean([log['prediction_std'] for log in recent_logs])),
                    'min': float(np.min([log['prediction_min'] for log in recent_logs])),
                    'max': float(np.max([log['prediction_max'] for log in recent_logs]))
                }
            },
            'alerts': {
                'total_alerts': len(self.state['drift_alerts']),
                'recent_alerts': self.state['drift_alerts'][-10:],
                'alert_types': self._count_alert_types()
            },
            'health_status': self._calculate_health_status()
        }
    
    def _count_alert_types(self):
        """Count alerts by type"""
        counts = {}
        for alert in self.state['drift_alerts']:
            alert_type = alert['type']
            counts[alert_type] = counts.get(alert_type, 0) + 1
        return counts
    
    def _calculate_health_status(self):
        """Calculate overall model health"""
        recent_alerts = self.state['drift_alerts'][-10:]
        
        if not recent_alerts:
            return 'healthy'
        
        high_severity = sum(1 for a in recent_alerts if a['severity'] == 'high')
        
        if high_severity >= 3:
            return 'critical'
        elif high_severity >= 1:
            return 'warning'
        else:
            return 'caution'


# Example usage
if __name__ == "__main__":
    from sklearn.datasets import make_regression
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split
    
    # Generate data
    X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # Train model
    model = RandomForestRegressor(random_state=42)
    model.fit(X_train, y_train)
    
    # Initialize monitoring
    monitor = ModelMonitor(model_id='test_model_001')
    
    # Set baseline
    monitor.set_baseline(X_train, y_train, feature_names=[f'feature_{i}' for i in range(5)])
    
    # Make predictions and log
    predictions = model.predict(X_test)
    monitor.log_predictions(X_test, predictions, actuals=y_test)
    
    # Simulate drift (shift features)
    X_drifted = X_test + 2.0
    drift_result = monitor.detect_data_drift(X_drifted)
    
    print(f"Drift detected: {drift_result['drift_detected']}")
    print(f"Drifted features: {drift_result['n_features_drifted']}")
    
    # Get dashboard
    dashboard = monitor.get_monitoring_dashboard()
    print(f"Health status: {dashboard['health_status']}")
