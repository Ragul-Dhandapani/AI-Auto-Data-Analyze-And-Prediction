#!/usr/bin/env python3
"""
SRE-Style Forecasting Feature Testing for PROMISE AI Platform
Tests the SRE forecasting feature in holistic analysis
"""

import requests
import json
import sys
import time
from typing import Dict, List, Any

# Backend URL from environment
BACKEND_URL = "https://promise-ai-1.preview.emergentagent.com/api"

class SREForecastTester:
    def __init__(self):
        self.backend_url = BACKEND_URL
        self.test_results = []
        self.test_dataset_id = None
        
    def log_test(self, test_name: str, status: str, details: str = "", response_data: Dict = None):
        """Log test results"""
        result = {
            "test": test_name,
            "status": status,
            "details": details,
            "response_data": response_data
        }
        self.test_results.append(result)
        print(f"{'âœ…' if status == 'PASS' else 'âŒ' if status == 'FAIL' else 'ðŸŸ¡'} {test_name}: {status}")
        if details:
            print(f"   Details: {details}")
        if status == "FAIL" and response_data:
            print(f"   Response: {response_data}")
        print()

    def get_available_datasets(self) -> List[Dict]:
        """Get list of available datasets"""
        try:
            response = requests.get(f"{self.backend_url}/datasets", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return data.get("datasets", [])
            else:
                return []
        except Exception as e:
            print(f"Failed to get datasets: {str(e)}")
            return []

    def upload_test_dataset(self) -> str:
        """Upload a small test dataset for testing"""
        # Create a small CSV dataset for testing
        test_csv_content = """cpu_usage,memory_usage,latency_ms,status
85.2,67.4,245,success
92.1,78.9,312,success
76.3,45.2,189,success
88.7,82.1,398,error
79.4,56.8,234,success
91.2,89.3,456,error
73.8,41.7,167,success
86.5,72.6,289,success
94.3,91.2,523,error
81.7,59.4,201,success"""

        try:
            files = {
                'file': ('test_latency_data.csv', test_csv_content, 'text/csv')
            }
            
            response = requests.post(
                f"{self.backend_url}/datasource/upload",
                files=files,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                dataset_id = data.get("dataset_id")
                print(f"âœ… Test dataset uploaded successfully: {dataset_id}")
                return dataset_id
            else:
                print(f"âŒ Failed to upload test dataset: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ Exception during dataset upload: {str(e)}")
            return None

    def run_holistic_analysis(self, dataset_id: str, user_selection: Dict = None) -> Dict:
        """Run holistic analysis with optional user selection"""
        payload = {
            "dataset_id": dataset_id,
            "workspace_name": "test_workspace_user_expectation",
            "problem_type": "auto"
        }
        
        if user_selection:
            payload["user_selection"] = user_selection
        
        try:
            response = requests.post(
                f"{self.backend_url}/analysis/holistic",
                json=payload,
                timeout=60  # Longer timeout for analysis
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {
                    "error": f"HTTP {response.status_code}",
                    "details": response.text
                }
        except Exception as e:
            return {
                "error": "Request failed",
                "details": str(e)
            }

    def test_setup_dataset(self):
        """Test 1: Setup test dataset"""
        # First try to find existing dataset with direct storage (not GridFS)
        datasets = self.get_available_datasets()
        
        # Look for a suitable dataset with direct storage (to avoid GridFS issues)
        suitable_dataset = None
        for dataset in datasets:
            name = dataset.get("name", "").lower()
            storage_type = dataset.get("storage_type", "")
            if ("test_data" in name or "latency" in name) and storage_type == "direct":
                suitable_dataset = dataset
                break
        
        if suitable_dataset:
            self.test_dataset_id = suitable_dataset.get("id")
            self.log_test("Setup Test Dataset", "PASS", 
                         f"Using existing dataset: {suitable_dataset.get('name')} (ID: {self.test_dataset_id})")
        else:
            # Upload test dataset
            self.test_dataset_id = self.upload_test_dataset()
            if self.test_dataset_id:
                self.log_test("Setup Test Dataset", "PASS", 
                             f"Uploaded new test dataset (ID: {self.test_dataset_id})")
            else:
                self.log_test("Setup Test Dataset", "FAIL", "Could not setup test dataset")

    def test_analysis_without_user_expectation(self):
        """Test 2: Analysis WITHOUT user expectation (baseline)"""
        if not self.test_dataset_id:
            self.log_test("Analysis Without User Expectation", "SKIP", "No test dataset available")
            return None
        
        print("ðŸ” Running baseline analysis without user expectation...")
        response = self.run_holistic_analysis(self.test_dataset_id)
        
        if "error" in response:
            self.log_test("Analysis Without User Expectation", "FAIL", 
                         f"Error: {response['error']}", response)
            return None
        
        # Check if analysis completed successfully
        if "insights" in response and "ml_models" in response:
            insights = response.get("insights", "")
            models = response.get("ml_models", [])
            
            self.log_test("Analysis Without User Expectation", "PASS", 
                         f"Analysis completed. Generated {len(models)} models and insights ({len(insights)} chars)")
            
            # Store baseline for comparison
            self.baseline_response = response
            return response
        else:
            self.log_test("Analysis Without User Expectation", "FAIL", 
                         "Analysis did not return expected results", response)
            return None

    def test_analysis_with_sre_forecast_generation(self):
        """Test 3: Analysis WITH SRE Forecast Generation (CRITICAL TEST)"""
        if not self.test_dataset_id:
            self.log_test("Analysis With SRE Forecast Generation", "SKIP", "No test dataset available")
            return None
        
        # Define user selection with expectation for SRE forecasting
        user_selection = {
            "target_variable": "latency_ms",
            "selected_features": ["cpu_usage", "memory_usage"],
            "mode": "manual",
            "user_expectation": "Predict system latency to prevent performance degradation"
        }
        
        print("ðŸ” Running analysis WITH SRE forecast generation...")
        response = self.run_holistic_analysis(self.test_dataset_id, user_selection)
        
        if "error" in response:
            self.log_test("Analysis With SRE Forecast Generation", "FAIL", 
                         f"Error: {response['error']}", response)
            return None
        
        # Check if analysis completed successfully
        if "insights" in response and "ml_models" in response:
            models = response.get("ml_models", [])
            
            # CRITICAL: Check for sre_forecast field
            sre_forecast = response.get("sre_forecast")
            
            if sre_forecast:
                self.log_test("Analysis With SRE Forecast Generation", "PASS", 
                             f"âœ… SRE FORECAST FOUND: Generated {len(models)} models with SRE forecasting.")
                
                # Store for detailed validation
                self.sre_forecast_response = response
                return response
            else:
                self.log_test("Analysis With SRE Forecast Generation", "FAIL", 
                             f"âŒ SRE FORECAST MISSING: Analysis completed with {len(models)} models but no sre_forecast field.")
                return None
        else:
            self.log_test("Analysis With SRE Forecast Generation", "FAIL", 
                         "Analysis did not return expected results", response)
            return None

    def test_sre_forecast_content_validation(self):
        """Test 4: Validate SRE Forecast Content Structure"""
        if not hasattr(self, 'sre_forecast_response'):
            self.log_test("SRE Forecast Content Validation", "SKIP", "No SRE forecast response available")
            return
        
        sre_forecast = self.sre_forecast_response.get("sre_forecast", {})
        
        # Check required fields
        required_fields = ["forecasts", "critical_alerts", "recommendations"]
        missing_fields = [field for field in required_fields if field not in sre_forecast]
        
        if missing_fields:
            self.log_test("SRE Forecast Content Validation", "FAIL", 
                         f"Missing required fields: {missing_fields}")
            return
        
        # Validate forecasts structure
        forecasts = sre_forecast.get("forecasts", [])
        forecast_valid = True
        forecast_details = []
        
        for i, forecast in enumerate(forecasts):
            required_forecast_fields = ["timeframe", "prediction", "value", "confidence"]
            missing_forecast_fields = [field for field in required_forecast_fields if field not in forecast]
            if missing_forecast_fields:
                forecast_valid = False
                forecast_details.append(f"Forecast {i+1} missing: {missing_forecast_fields}")
            else:
                forecast_details.append(f"Forecast {i+1}: {forecast['timeframe']} - {forecast['prediction']}")
        
        # Validate alerts structure
        alerts = sre_forecast.get("critical_alerts", [])
        alert_valid = True
        alert_details = []
        
        for i, alert in enumerate(alerts):
            required_alert_fields = ["severity", "alert"]
            missing_alert_fields = [field for field in required_alert_fields if field not in alert]
            if missing_alert_fields:
                alert_valid = False
                alert_details.append(f"Alert {i+1} missing: {missing_alert_fields}")
            else:
                alert_details.append(f"Alert {i+1}: {alert['severity']} - {alert['alert']}")
        
        # Validate recommendations structure
        recommendations = sre_forecast.get("recommendations", [])
        rec_valid = True
        rec_details = []
        
        for i, rec in enumerate(recommendations):
            required_rec_fields = ["priority", "action"]
            missing_rec_fields = [field for field in required_rec_fields if field not in rec]
            if missing_rec_fields:
                rec_valid = False
                rec_details.append(f"Recommendation {i+1} missing: {missing_rec_fields}")
            else:
                rec_details.append(f"Recommendation {i+1}: {rec['priority']} - {rec['action']}")
        
        # Overall validation
        if forecast_valid and alert_valid and rec_valid:
            self.log_test("SRE Forecast Content Validation", "PASS", 
                         f"âœ… All structures valid: {len(forecasts)} forecasts, {len(alerts)} alerts, {len(recommendations)} recommendations")
            print(f"   ðŸ“Š Forecasts: {forecast_details}")
            print(f"   ðŸš¨ Alerts: {alert_details}")
            print(f"   ðŸ’¡ Recommendations: {rec_details}")
        else:
            issues = []
            if not forecast_valid:
                issues.extend(forecast_details)
            if not alert_valid:
                issues.extend(alert_details)
            if not rec_valid:
                issues.extend(rec_details)
            
            self.log_test("SRE Forecast Content Validation", "FAIL", 
                         f"Structure validation failed: {'; '.join(issues)}")

    def test_backend_logging_verification(self):
        """Test 5: Check Backend Logging for SRE Forecast Generation"""
        # Since we can't directly access logs, we'll check for indicators in the response
        # that suggest proper logging is happening
        
        if not hasattr(self, 'sre_forecast_response'):
            self.log_test("Backend Logging Verification", "SKIP", "No SRE forecast response available")
            return
        
        response = self.sre_forecast_response
        
        # Check if we have models (indicates training completed)
        models = response.get("ml_models", [])
        sre_forecast = response.get("sre_forecast", {})
        
        # If we have models and SRE forecast, logging should have occurred
        if models and sre_forecast:
            forecasts_count = len(sre_forecast.get("forecasts", []))
            alerts_count = len(sre_forecast.get("critical_alerts", []))
            
            self.log_test("Backend Logging Verification", "PASS", 
                         f"âœ… SRE forecast generation completed: {forecasts_count} forecasts, {alerts_count} alerts (logging should show: 'ðŸ”® Generating SRE-style forecast summaries...' and 'âœ… SRE forecast generated')")
        else:
            self.log_test("Backend Logging Verification", "FAIL", 
                         "SRE forecast generation may not have completed properly")

    def test_azure_openai_integration(self):
        """Test 6: Azure OpenAI integration status"""
        if not hasattr(self, 'expectation_response'):
            self.log_test("Azure OpenAI Integration", "SKIP", "No expectation response available")
            return
        
        insights = self.expectation_response.get("insights", "")
        
        # Check for AI-generated content characteristics
        ai_indicators = [
            "recommendation", "analysis", "findings", "performance", 
            "model", "data", "business", "actionable"
        ]
        
        ai_score = sum(1 for indicator in ai_indicators if indicator.lower() in insights.lower())
        
        if ai_score >= 4 and len(insights) > 200:
            self.log_test("Azure OpenAI Integration", "PASS", 
                         f"âœ… AI-powered insights detected (score: {ai_score}/8, length: {len(insights)} chars)")
        elif len(insights) > 50:
            self.log_test("Azure OpenAI Integration", "PARTIAL", 
                         f"Basic insights generated (score: {ai_score}/8, length: {len(insights)} chars)")
        else:
            self.log_test("Azure OpenAI Integration", "FAIL", 
                         "Minimal or no insights generated - Azure OpenAI may not be configured")

    def test_error_handling(self):
        """Test 7: Error handling with invalid parameters"""
        if not self.test_dataset_id:
            self.log_test("Error Handling", "SKIP", "No test dataset available")
            return
        
        # Test with invalid dataset ID
        invalid_response = self.run_holistic_analysis("invalid-dataset-id")
        
        if "error" in invalid_response:
            self.log_test("Error Handling", "PASS", 
                         f"Proper error handling for invalid dataset: {invalid_response['error']}")
        else:
            self.log_test("Error Handling", "FAIL", 
                         "No error returned for invalid dataset ID")

    def run_all_tests(self):
        """Run all tests in sequence"""
        print("ðŸ§ª Enhanced User Expectation Feature Testing for PROMISE AI Platform")
        print("=" * 80)
        print(f"Backend URL: {self.backend_url}")
        print()
        
        # Run tests in order
        self.test_setup_dataset()
        self.test_analysis_without_user_expectation()
        self.test_analysis_with_user_expectation()
        self.test_backend_logging_verification()
        self.test_insights_comparison()
        self.test_azure_openai_integration()
        self.test_error_handling()
        
        # Summary
        print("=" * 80)
        print("ðŸ“Š TEST SUMMARY")
        print("=" * 80)
        
        passed = sum(1 for r in self.test_results if r["status"] == "PASS")
        failed = sum(1 for r in self.test_results if r["status"] == "FAIL")
        partial = sum(1 for r in self.test_results if r["status"] == "PARTIAL")
        skipped = sum(1 for r in self.test_results if r["status"] == "SKIP")
        total = len(self.test_results)
        
        print(f"Total Tests: {total}")
        print(f"âœ… Passed: {passed}")
        print(f"âŒ Failed: {failed}")
        print(f"ðŸŸ¡ Partial: {partial}")
        print(f"â­ï¸  Skipped: {skipped}")
        print()
        
        success_rate = ((passed + partial * 0.5) / total) * 100 if total > 0 else 0
        print(f"Success Rate: {success_rate:.1f}%")
        
        # Critical findings
        critical_failures = [r for r in self.test_results if r["status"] == "FAIL"]
        if critical_failures:
            print("\nðŸš¨ CRITICAL ISSUES:")
            for failure in critical_failures:
                print(f"   - {failure['test']}: {failure['details']}")
        
        # User expectation feature status
        expectation_tests = [r for r in self.test_results if "Expectation" in r["test"] or "Comparison" in r["test"]]
        expectation_working = sum(1 for r in expectation_tests if r["status"] in ["PASS", "PARTIAL"])
        
        print(f"\nðŸŽ¯ USER EXPECTATION FEATURE STATUS:")
        if expectation_working >= len(expectation_tests) * 0.7:  # 70% threshold
            print("   âœ… User expectation feature is working")
        else:
            print("   âŒ User expectation feature needs attention")
        
        # Azure OpenAI status
        ai_tests = [r for r in self.test_results if "Azure" in r["test"] or "AI" in r["test"]]
        ai_working = any(r["status"] == "PASS" for r in ai_tests)
        
        print(f"\nðŸ¤– AZURE OPENAI STATUS:")
        if ai_working:
            print("   âœ… Azure OpenAI integration is working")
        else:
            print("   âš ï¸  Azure OpenAI may not be configured (acceptable)")
        
        return {
            "total": total,
            "passed": passed,
            "failed": failed,
            "partial": partial,
            "success_rate": success_rate,
            "expectation_working": expectation_working >= len(expectation_tests) * 0.7,
            "ai_working": ai_working
        }

def main():
    """Main test execution"""
    tester = UserExpectationTester()
    results = tester.run_all_tests()
    
    # Exit with appropriate code
    if results["failed"] > 0:
        sys.exit(1)
    else:
        sys.exit(0)

if __name__ == "__main__":
    main()
