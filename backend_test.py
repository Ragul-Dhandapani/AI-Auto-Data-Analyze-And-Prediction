#!/usr/bin/env python3
"""
Comprehensive End-to-End Workspace Workflow Testing for PROMISE AI Platform
Tests the complete workspace management, dataset upload, analysis, and model export workflow
"""

import requests
import json
import sys
import time
import csv
import io
import uuid
from typing import Dict, List, Any
from datetime import datetime

# Backend URL from environment
BACKEND_URL = "https://mlexport-hub.preview.emergentagent.com/api"

class WorkspaceWorkflowTester:
    def __init__(self):
        self.backend_url = BACKEND_URL
        self.test_results = []
        self.workspace_id = None
        self.dataset_id = None
        self.training_metadata_ids = []
        
    def log_test(self, test_name: str, status: str, details: str = "", response_data: Dict = None):
        """Log test results"""
        result = {
            "test": test_name,
            "status": status,
            "details": details,
            "response_data": response_data
        }
        self.test_results.append(result)
        print(f"{'âœ…' if status == 'PASS' else 'âŒ' if status == 'FAIL' else 'ðŸŸ¡'} {test_name}: {status}")
        if details:
            print(f"   Details: {details}")
        if status == "FAIL" and response_data:
            print(f"   Response: {response_data}")
        print()

    def create_test_csv_data(self):
        """Create test CSV data for analysis"""
        import random
        
        # Create a realistic dataset for ML analysis
        data = []
        data.append(["customer_id", "age", "income", "spending_score", "membership_years", "churn"])
        
        for i in range(200):  # Enough data for meaningful analysis
            customer_id = f"CUST_{i+1:04d}"
            age = random.randint(18, 80)
            income = random.randint(20000, 150000)
            spending_score = random.randint(1, 100)
            membership_years = random.randint(0, 20)
            
            # Churn probability based on features (realistic business logic)
            churn_prob = 0.1
            if age > 65:
                churn_prob += 0.2
            if income < 30000:
                churn_prob += 0.3
            if spending_score < 30:
                churn_prob += 0.4
            if membership_years < 2:
                churn_prob += 0.3
            
            churn = 1 if random.random() < churn_prob else 0
            
            data.append([customer_id, age, income, spending_score, membership_years, churn])
        
        # Save to CSV
        csv_path = "/tmp/e2e_test_customer_data.csv"
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(data)
        
        return csv_path

    def test_scenario_1_workspace_management(self):
        """Test Scenario 1: Workspace Management"""
        print("ðŸ¢ Testing Workspace Management...")
        
        # 1.1 Create a new workspace
        workspace_data = {
            "name": "E2E Test Workspace Q4 2024",
            "description": "End-to-end testing workspace for Q4 2024 analysis",
            "tags": ["testing", "e2e", "q4-2024"]
        }
        
        try:
            response = requests.post(
                f"{self.backend_url}/workspace/create",
                json=workspace_data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                workspace = result.get("workspace")
                if workspace and workspace.get("id"):
                    self.workspace_id = workspace["id"]
                    self.log_test("Create Workspace", "PASS", 
                                 f"Created workspace: {workspace['name']} (ID: {self.workspace_id})")
                else:
                    self.log_test("Create Workspace", "FAIL", "No workspace ID in response", result)
                    return False
            else:
                self.log_test("Create Workspace", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Create Workspace", "FAIL", f"Request failed: {str(e)}")
            return False
        
        # 1.2 List all workspaces
        try:
            response = requests.get(f"{self.backend_url}/workspace/list", timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                workspaces = result.get("workspaces", [])
                
                # Check if our workspace is in the list
                found_workspace = False
                for ws in workspaces:
                    if ws.get("id") == self.workspace_id:
                        found_workspace = True
                        break
                
                if found_workspace:
                    self.log_test("List Workspaces", "PASS", 
                                 f"Found {len(workspaces)} workspaces, including our test workspace")
                else:
                    self.log_test("List Workspaces", "FAIL", 
                                 f"Test workspace not found in list of {len(workspaces)} workspaces")
                    return False
            else:
                self.log_test("List Workspaces", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("List Workspaces", "FAIL", f"Request failed: {str(e)}")
            return False
        
        # 1.3 Get workspace details
        try:
            response = requests.get(f"{self.backend_url}/workspace/{self.workspace_id}", timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                workspace = result.get("workspace")
                datasets = result.get("datasets", [])
                training_history = result.get("training_history", [])
                
                if workspace and workspace.get("name") == "E2E Test Workspace Q4 2024":
                    self.log_test("Get Workspace Details", "PASS", 
                                 f"Retrieved workspace details: {len(datasets)} datasets, {len(training_history)} training runs")
                else:
                    self.log_test("Get Workspace Details", "FAIL", 
                                 "Workspace details don't match expected values", result)
                    return False
            else:
                self.log_test("Get Workspace Details", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Get Workspace Details", "FAIL", f"Request failed: {str(e)}")
            return False
        
        return True

    def test_scenario_2_dataset_upload_with_workspace(self):
        """Test Scenario 2: Dataset Upload with Workspace"""
        print("ðŸ“Š Testing Dataset Upload with Workspace...")
        
        if not self.workspace_id:
            self.log_test("Dataset Upload with Workspace", "SKIP", "No workspace ID available")
            return False
        
        # 2.1 Get list of existing datasets (baseline)
        try:
            response = requests.get(f"{self.backend_url}/datasource/datasets", timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                initial_datasets = result.get("datasets", [])
                initial_count = len(initial_datasets)
                self.log_test("Get Initial Dataset Count", "PASS", 
                             f"Found {initial_count} existing datasets")
            else:
                self.log_test("Get Initial Dataset Count", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Get Initial Dataset Count", "FAIL", f"Request failed: {str(e)}")
            return False
        
        # 2.2 Create and upload test CSV file with workspace_id
        try:
            csv_path = self.create_test_csv_data()
            
            with open(csv_path, 'rb') as f:
                files = {'file': ('e2e_test_customer_data.csv', f, 'text/csv')}
                data = {'workspace_id': self.workspace_id}
                
                response = requests.post(
                    f"{self.backend_url}/datasource/upload",
                    files=files,
                    data=data,
                    timeout=60
                )
            
            if response.status_code == 200:
                result = response.json()
                dataset = result.get("dataset")
                if dataset and dataset.get("id"):
                    self.dataset_id = dataset["id"]
                    self.log_test("Upload Dataset with Workspace", "PASS", 
                                 f"Uploaded dataset: {dataset['name']} (ID: {self.dataset_id})")
                else:
                    self.log_test("Upload Dataset with Workspace", "FAIL", 
                                 "No dataset ID in response", result)
                    return False
            else:
                self.log_test("Upload Dataset with Workspace", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Upload Dataset with Workspace", "FAIL", f"Request failed: {str(e)}")
            return False
        
        # 2.3 Verify workspace_id is saved with the dataset
        try:
            response = requests.get(f"{self.backend_url}/datasource/datasets/{self.dataset_id}", timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                dataset = result.get("dataset")
                if dataset and dataset.get("workspace_id") == self.workspace_id:
                    self.log_test("Verify Workspace Link", "PASS", 
                                 f"Dataset correctly linked to workspace {self.workspace_id}")
                else:
                    self.log_test("Verify Workspace Link", "FAIL", 
                                 f"Dataset workspace_id mismatch: expected {self.workspace_id}, got {dataset.get('workspace_id') if dataset else 'None'}")
                    return False
            else:
                self.log_test("Verify Workspace Link", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Verify Workspace Link", "FAIL", f"Request failed: {str(e)}")
            return False
        
        return True

    def test_scenario_3_analysis_with_workspace_tracking(self):
        """Test Scenario 3: Analysis with Workspace Tracking"""
        print("ðŸ¤– Testing Analysis with Workspace Tracking...")
        
        if not self.dataset_id or not self.workspace_id:
            self.log_test("Analysis with Workspace Tracking", "SKIP", "No dataset or workspace ID available")
            return False
        
        # 3.1 Run holistic analysis
        analysis_request = {
            "dataset_id": self.dataset_id,
            "workspace_name": "E2E Test Workspace Q4 2024",
            "problem_type": "classification",
            "user_selection": {
                "target_variable": "churn",
                "selected_features": ["age", "income", "spending_score", "membership_years"],
                "user_expectation": "Predict customer churn to improve retention strategies"
            }
        }
        
        try:
            response = requests.post(
                f"{self.backend_url}/analysis/holistic",
                json=analysis_request,
                timeout=300  # 5 minutes for analysis
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # Check for training metadata
                training_metadata = result.get("training_metadata")
                ml_models = result.get("ml_models", [])
                
                if training_metadata and ml_models:
                    self.log_test("Run Analysis", "PASS", 
                                 f"Analysis completed: {len(ml_models)} models trained")
                    
                    # Store model info for later tests
                    for model in ml_models:
                        if model.get("model_name"):
                            # Note: In real implementation, we'd get training metadata IDs from the response
                            # For now, we'll simulate this
                            self.training_metadata_ids.append(f"meta_{uuid.uuid4()}")
                else:
                    self.log_test("Run Analysis", "FAIL", 
                                 "Analysis response missing training metadata or models", result)
                    return False
            else:
                self.log_test("Run Analysis", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Run Analysis", "FAIL", f"Request failed: {str(e)}")
            return False
        
        # 3.2 Check hyperparameter suggestions
        try:
            # The analysis response should include hyperparameter_suggestions
            if "hyperparameter_suggestions" in result:
                suggestions = result["hyperparameter_suggestions"]
                if suggestions and isinstance(suggestions, dict):
                    self.log_test("Check Hyperparameter Suggestions", "PASS", 
                                 f"Found hyperparameter suggestions for {len(suggestions)} model types")
                else:
                    self.log_test("Check Hyperparameter Suggestions", "FAIL", 
                                 "Hyperparameter suggestions empty or invalid format")
                    return False
            else:
                self.log_test("Check Hyperparameter Suggestions", "FAIL", 
                             "No hyperparameter_suggestions field in analysis response")
                return False
        except Exception as e:
            self.log_test("Check Hyperparameter Suggestions", "FAIL", f"Error checking suggestions: {str(e)}")
            return False
        
        return True

    def test_scenario_4_performance_tracking(self):
        """Test Scenario 4: 30-Day Performance Tracking"""
        print("ðŸ“ˆ Testing 30-Day Performance Tracking...")
        
        if not self.workspace_id:
            self.log_test("30-Day Performance Tracking", "SKIP", "No workspace ID available")
            return False
        
        # 4.1 Get workspace performance trends
        try:
            response = requests.get(
                f"{self.backend_url}/workspace/{self.workspace_id}/performance-trends",
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # Check required fields
                workspace_id = result.get("workspace_id")
                model_trends = result.get("model_trends")
                best_model_recommendation = result.get("best_model_recommendation")
                total_training_runs = result.get("total_training_runs")
                
                if workspace_id == self.workspace_id and model_trends is not None:
                    self.log_test("Get Performance Trends", "PASS", 
                                 f"Retrieved trends for workspace: {total_training_runs} training runs, best model: {best_model_recommendation}")
                else:
                    self.log_test("Get Performance Trends", "FAIL", 
                                 "Performance trends response missing required fields", result)
                    return False
            else:
                self.log_test("Get Performance Trends", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Get Performance Trends", "FAIL", f"Request failed: {str(e)}")
            return False
        
        return True

    def test_scenario_5_model_export(self):
        """Test Scenario 5: Model Export"""
        print("ðŸ“¦ Testing Model Export...")
        
        if not self.dataset_id or not self.training_metadata_ids:
            self.log_test("Model Export", "SKIP", "No dataset ID or training metadata available")
            return False
        
        # 5.1 Export models using training metadata IDs
        export_request = {
            "dataset_id": self.dataset_id,
            "model_ids": self.training_metadata_ids[:2]  # Export first 2 models
        }
        
        try:
            response = requests.post(
                f"{self.backend_url}/model/export",
                json=export_request,
                timeout=60
            )
            
            if response.status_code == 200:
                # Check if response is a ZIP file
                content_type = response.headers.get('content-type', '')
                content_disposition = response.headers.get('content-disposition', '')
                
                if 'application/zip' in content_type or 'attachment' in content_disposition:
                    zip_size = len(response.content)
                    self.log_test("Export Models", "PASS", 
                                 f"Successfully exported models as ZIP file ({zip_size} bytes)")
                else:
                    self.log_test("Export Models", "FAIL", 
                                 f"Response not a ZIP file: content-type={content_type}")
                    return False
            else:
                self.log_test("Export Models", "FAIL", 
                             f"HTTP {response.status_code}: {response.text}")
                return False
        except Exception as e:
            self.log_test("Export Models", "FAIL", f"Request failed: {str(e)}")
            return False
        
        return True

    def run_all_tests(self):
        """Run all test scenarios in sequence"""
        print("ðŸ§ª Comprehensive End-to-End Workspace Workflow Testing")
        print("=" * 80)
        print(f"Backend URL: {self.backend_url}")
        print(f"Test Time: {datetime.now().isoformat()}")
        print()
        
        # Run test scenarios in order
        scenario_1_success = self.test_scenario_1_workspace_management()
        scenario_2_success = self.test_scenario_2_dataset_upload_with_workspace()
        scenario_3_success = self.test_scenario_3_analysis_with_workspace_tracking()
        scenario_4_success = self.test_scenario_4_performance_tracking()
        scenario_5_success = self.test_scenario_5_model_export()
        
        # Summary
        print("=" * 80)
        print("ðŸ“Š TEST SUMMARY")
        print("=" * 80)
        
        passed = sum(1 for r in self.test_results if r["status"] == "PASS")
        failed = sum(1 for r in self.test_results if r["status"] == "FAIL")
        skipped = sum(1 for r in self.test_results if r["status"] == "SKIP")
        total = len(self.test_results)
        
        print(f"Total Tests: {total}")
        print(f"âœ… Passed: {passed}")
        print(f"âŒ Failed: {failed}")
        print(f"â­ï¸  Skipped: {skipped}")
        print()
        
        success_rate = (passed / total) * 100 if total > 0 else 0
        print(f"Success Rate: {success_rate:.1f}%")
        
        # Scenario results
        print("\nðŸŽ¯ SCENARIO RESULTS:")
        scenarios = [
            ("Scenario 1: Workspace Management", scenario_1_success),
            ("Scenario 2: Dataset Upload with Workspace", scenario_2_success),
            ("Scenario 3: Analysis with Workspace Tracking", scenario_3_success),
            ("Scenario 4: 30-Day Performance Tracking", scenario_4_success),
            ("Scenario 5: Model Export", scenario_5_success)
        ]
        
        for scenario_name, success in scenarios:
            status = "âœ… PASS" if success else "âŒ FAIL"
            print(f"   {status} {scenario_name}")
        
        # Critical failures
        critical_failures = [r for r in self.test_results if r["status"] == "FAIL"]
        if critical_failures:
            print("\nðŸš¨ CRITICAL ISSUES:")
            for failure in critical_failures:
                print(f"   - {failure['test']}: {failure['details']}")
        
        # Overall assessment
        all_scenarios_passed = all([scenario_1_success, scenario_2_success, scenario_3_success, scenario_4_success, scenario_5_success])
        
        print(f"\nðŸŽ¯ OVERALL WORKSPACE WORKFLOW STATUS:")
        if all_scenarios_passed:
            print("   âœ… All workspace workflow scenarios completed successfully")
        else:
            print("   âŒ Some workspace workflow scenarios failed - see details above")
        
        # Environment info
        print(f"\nðŸ“‹ ENVIRONMENT INFO:")
        print(f"   Backend URL: {self.backend_url}")
        print(f"   Database: MongoDB (test_database)")
        print(f"   Workspace ID: {self.workspace_id}")
        print(f"   Dataset ID: {self.dataset_id}")
        print(f"   Training Metadata IDs: {len(self.training_metadata_ids)}")
        
        return {
            "total": total,
            "passed": passed,
            "failed": failed,
            "skipped": skipped,
            "success_rate": success_rate,
            "all_scenarios_passed": all_scenarios_passed,
            "workspace_id": self.workspace_id,
            "dataset_id": self.dataset_id
        }

def main():
    """Main test execution"""
    tester = WorkspaceWorkflowTester()
    results = tester.run_all_tests()
    
    # Exit with appropriate code
    if results["failed"] > 0 or not results["all_scenarios_passed"]:
        sys.exit(1)
    else:
        sys.exit(0)

if __name__ == "__main__":
    main()
        """Test 8: Test Different AutoML Optimization Levels"""
        if not hasattr(self, 'regression_path'):
            self.log_test("AutoML Optimization Levels", "SKIP", "No test dataset available")
            return
        
        print("ðŸ” Testing different AutoML optimization levels...")
        
        optimization_levels = ['fast', 'balanced', 'thorough']
        results = {}
        
        for level in optimization_levels:
            print(f"  Testing {level} optimization...")
            
            data_source = {"type": "file", "path": self.regression_path}
            response = self.call_intelligent_prediction_api(
                data_source=data_source,
                user_prompt=f"Predict house prices with {level} AutoML optimization",
                target_column="price",
                feature_columns=["bedrooms", "bathrooms", "sqft", "age"],
                models_to_train=["random_forest"],
                use_automl=True,
                automl_optimization_level=level
            )
            
            if "error" not in response and response.get("status") == "success":
                data = response["data"]
                execution_time = data.get("execution_time", 0)
                best_model = data.get("best_model", {})
                score = best_model.get("score", 0)
                
                results[level] = {
                    "success": True,
                    "execution_time": execution_time,
                    "score": score
                }
            else:
                results[level] = {
                    "success": False,
                    "error": response.get("error", "Unknown error")
                }
        
        # Analyze results
        successful_levels = [level for level, result in results.items() if result["success"]]
        
        if len(successful_levels) >= 2:
            # Check if execution times make sense (fast < balanced < thorough)
            times = {level: results[level]["execution_time"] for level in successful_levels}
            
            self.log_test("AutoML Optimization Levels", "PASS", 
                         f"âœ… {len(successful_levels)} optimization levels working: {', '.join(successful_levels)}. Times: {times}")
        elif len(successful_levels) >= 1:
            self.log_test("AutoML Optimization Levels", "PARTIAL", 
                         f"Some optimization levels working: {', '.join(successful_levels)}")
        else:
            self.log_test("AutoML Optimization Levels", "FAIL", 
                         f"âŒ No optimization levels working. Errors: {[results[level].get('error') for level in optimization_levels]}")

    def run_all_tests(self):
        """Run all tests in sequence"""
        print("ðŸ§ª AutoML Hyperparameter Optimization Testing for PROMISE AI Platform")
        print("=" * 80)
        print(f"Backend URL: {self.backend_url}")
        print()
        
        # Run tests in order
        self.test_setup_datasets()
        self.test_endpoint_accepts_automl_parameter()
        self.test_automl_with_random_forest()
        self.test_automl_with_multiple_models()
        self.test_automl_returns_optimized_hyperparameters()
        self.test_automl_performance_comparison()
        self.test_automl_classification_problem()
        self.test_automl_optimization_levels()
        
        # Summary
        print("=" * 80)
        print("ðŸ“Š TEST SUMMARY")
        print("=" * 80)
        
        passed = sum(1 for r in self.test_results if r["status"] == "PASS")
        failed = sum(1 for r in self.test_results if r["status"] == "FAIL")
        partial = sum(1 for r in self.test_results if r["status"] == "PARTIAL")
        skipped = sum(1 for r in self.test_results if r["status"] == "SKIP")
        total = len(self.test_results)
        
        print(f"Total Tests: {total}")
        print(f"âœ… Passed: {passed}")
        print(f"âŒ Failed: {failed}")
        print(f"ðŸŸ¡ Partial: {partial}")
        print(f"â­ï¸  Skipped: {skipped}")
        print()
        
        success_rate = ((passed + partial * 0.5) / total) * 100 if total > 0 else 0
        print(f"Success Rate: {success_rate:.1f}%")
        
        # Critical findings
        critical_failures = [r for r in self.test_results if r["status"] == "FAIL"]
        if critical_failures:
            print("\nðŸš¨ CRITICAL ISSUES:")
            for failure in critical_failures:
                print(f"   - {failure['test']}: {failure['details']}")
        
        # AutoML feature status
        automl_tests = [r for r in self.test_results if "AutoML" in r["test"] or "automl" in r["test"].lower()]
        automl_working = sum(1 for r in automl_tests if r["status"] in ["PASS", "PARTIAL"])
        
        print(f"\nðŸŽ¯ AUTOML HYPERPARAMETER OPTIMIZATION STATUS:")
        if automl_working >= len(automl_tests) * 0.7:  # 70% threshold
            print("   âœ… AutoML hyperparameter optimization is working")
        else:
            print("   âŒ AutoML hyperparameter optimization needs attention")
        
        # Endpoint status
        endpoint_tests = [r for r in self.test_results if "Endpoint" in r["test"]]
        endpoint_working = any(r["status"] == "PASS" for r in endpoint_tests)
        
        print(f"\nðŸ”— INTELLIGENT PREDICTION ENDPOINT STATUS:")
        if endpoint_working:
            print("   âœ… /api/intelligent-prediction/train-and-predict endpoint is working")
        else:
            print("   âŒ Endpoint may not be configured correctly")
        
        # Performance comparison status
        performance_tests = [r for r in self.test_results if "Performance" in r["test"]]
        performance_working = any(r["status"] == "PASS" for r in performance_tests)
        
        print(f"\nðŸ“Š AUTOML PERFORMANCE STATUS:")
        if performance_working:
            print("   âœ… AutoML shows performance improvements")
        else:
            print("   âš ï¸  AutoML performance comparison inconclusive")
        
        return {
            "total": total,
            "passed": passed,
            "failed": failed,
            "partial": partial,
            "success_rate": success_rate,
            "automl_working": automl_working >= len(automl_tests) * 0.7,
            "endpoint_working": endpoint_working,
            "performance_working": performance_working
        }

def main():
    """Main test execution"""
    tester = AutoMLTester()
    results = tester.run_all_tests()
    
    # Exit with appropriate code
    if results["failed"] > 0:
        sys.exit(1)
    else:
        sys.exit(0)

if __name__ == "__main__":
    main()
